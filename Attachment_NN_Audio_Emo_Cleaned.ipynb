{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["gctLlIZwvOzQ","7vWoCWxuvSH9","D79YFiCBvT-L","2vUp9fJdvV8y","t66-L3tYw1tF","UABcjee4w9vt","xy_zeRJy3WPa","Hjs89NWI3ZtO","933atyA2a-dM"],"machine_shape":"hm","mount_file_id":"16IZ_jMX-1SyvEcMXd7yOfootukELTBPR","authorship_tag":"ABX9TyPv5OnIGCgApYnWtunYUu7e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Install"],"metadata":{"id":"gctLlIZwvOzQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUbs4XP4vIB6","executionInfo":{"status":"ok","timestamp":1712169648138,"user_tz":-180,"elapsed":8930,"user":{"displayName":"Lab SocialSP","userId":"02146407728601162358"}},"outputId":"01586988-bff8-4698-95ea-4b6719d15b43"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras-self-attention\n","  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.25.2)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18894 sha256=caa552e3ae432ed0edc94b03983101a2f61b75d52e564e5b214a03dea369e757\n","  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.51.0\n"]}],"source":["pip install keras-self-attention"]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"7vWoCWxuvSH9"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import re\n","import os\n","from os import walk\n","import io\n","\n","import time\n","from timeit import default_timer as timer\n","from datetime import datetime\n","\n","import string\n","import numpy as np\n","import pandas as pd\n","import scipy\n","from scipy import stats\n","from scipy.stats import entropy\n","\n","from collections import Counter\n","\n","import statistics\n","from statistics import mean, mode, stdev, median\n","import math\n","\n","#from numpy import mean\n","from numpy import std\n","\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n","from sklearn.linear_model import SGDClassifier, RidgeClassifier\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn import datasets, neighbors\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.utils import shuffle\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.naive_bayes import MultinomialNB, ComplementNB, GaussianNB\n","#import xgboost as xgb\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.utils.extmath import softmax\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n","\n","import random\n","from random import seed, randint, shuffle\n","\n","import gensim\n","import nltk\n","from string import punctuation\n","from nltk import word_tokenize\n","from nltk.util import ngrams\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","\n","# nltk.download('stopwords')\n","\n","#from tqdm import tqdm, trange\n","from tqdm.auto import tqdm\n","\n","#from scikeras.wrappers import KerasClassifier\n","\n","# BERT imports\n","import tensorflow as tf\n","#from tensorflow.keras.callbacks import ModelCheckpoint\n","from keras.layers import Dense, Flatten, Conv1D, GlobalMaxPool1D, Conv2D, GlobalMaxPool2D, Attention\n","from keras.layers import Dropout, Softmax, Embedding, Input, concatenate, SimpleRNN, LSTM, Masking\n","from keras import Model, layers, models\n","from keras.models import Sequential\n","from keras.initializers import Constant\n","#from keras_preprocessing.sequence import pad_sequences\n","#from keras_preprocessing.text import one_hot\n","from keras.optimizers.legacy import Adam\n","import keras\n","from keras import backend as K\n","\n","from itertools import islice\n","\n","from keras_self_attention import SeqSelfAttention\n","\n","AUTOTUNE = tf.data.AUTOTUNE"],"metadata":{"id":"gktfx45HvOTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Seed value\n","# Apparently you may use different seed values at each stage\n","seed_value= 42\n","\n","# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","# 2. Set the `python` built-in pseudo-random generator at a fixed value\n","random.seed(seed_value)\n","\n","# 3. Set the `numpy` pseudo-random generator at a fixed value\n","np.random.seed(seed_value)\n","\n","# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n","tf.random.set_seed(seed_value)\n","# for later versions:\n","# tf.compat.v1.set_random_seed(seed_value)\n","\n","# 5. Configure a new global `tensorflow` session\n","session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","tf.compat.v1.keras.backend.set_session(sess)"],"metadata":{"id":"jwOBBmy_whmS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Drive"],"metadata":{"id":"D79YFiCBvT-L"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcY3oplfvOak","executionInfo":{"status":"ok","timestamp":1712169670347,"user_tz":-180,"elapsed":15389,"user":{"displayName":"Lab SocialSP","userId":"02146407728601162358"}},"outputId":"01e2b6e2-d817-4e9f-b3c5-b4a79055c85b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/Areej/Attachment Emotions/'"],"metadata":{"id":"jyPv4E5rwkJa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"2vUp9fJdvV8y"}},{"cell_type":"code","source":["data = pd.read_csv(path+'Audio/Data/attachment_emotions_audio_v2.csv', header=0)"],"metadata":{"id":"hbHFY295vOdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# rename some columns for consistency\n","data.rename({'ID': 'childID', 'Secure': 'insecure', 'Gender': 'gender', 'SchoolYear': 'schoolyear', 'Task': 'task',\n","                  'Path': 'path', 'Duration': 'duration', 'Start': 'start', 'End': 'end', 'Text': 'text'}, inplace=True, axis=1)"],"metadata":{"id":"rt2oBxTb48Z_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Switch the secure label to insecure, including the values (0, 1) instead of (-1, 1)\n","data.loc[data['insecure']==0, 'insecure'] = -1\n","data.loc[data['insecure']==1, 'insecure'] = 0\n","data.loc[data['insecure']==-1, 'insecure'] = 1"],"metadata":{"id":"ilzUCss45Ass"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop extra unneeded columns\n","data.drop(['gender', 'schoolyear', 'path', 'duration', 'start', 'end', 'text', 'time', 'segment'], axis=1, inplace=True)"],"metadata":{"id":"xFI5AYUs5AvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# columns to be dropped by the model during training\n","xDroppedColumns = ['childID', 'insecure', 'task']"],"metadata":{"id":"JC0ePEOR5Idw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calculations\n","\n","Calculating the performance results in accuracy, precision, recall and F1, on a story level and all data level"],"metadata":{"id":"t66-L3tYw1tF"}},{"cell_type":"code","source":["def get_max_count(arr):#, prob0, prob1):\n","    # create a Counter object from the input array\n","    count_dict = Counter(arr)\n","    # find the element(s) with the maximum count\n","    max_count = max(count_dict.values())\n","    max_elements = [k for k, v in count_dict.items() if v == max_count]\n","    if len(max_elements) == 1:\n","        # return the element with the maximum count\n","        return max_elements[0]\n","    else:\n","        # Multiple similarly counted element, get the largest\n","        return max(max_elements)\n","        #return None\n","        #return 0 if prob0 > prob1 else 1\n","\n","# Aggregate children's tasks MV\n","def getAggregatedTasksMVResults(resultsDF, isProb):\n","    aggTaskMVResDF = resultsDF.copy()\n","\n","    # Grouping the data based on task\n","    aggTaskMVResDF = aggTaskMVResDF.groupby(['task','child_id'])\n","\n","    def aggTaskMVData(data, isProb):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['task'] = data['task'].min()\n","        df['truth'] = int(data['truth'].min())\n","        df['prediction'] = get_max_count(data['prediction'].to_list())#, round(data['prob0'].mean(),2), round(data['prob1'].mean(),2))\n","\n","        pred1 = int(df['prediction'])\n","        pred2 = 0 if pred1 == 1 else 1\n","\n","        if isProb: # prob is calculated\n","            df['prob'+str(pred1)] = round(data['prob'+str(pred1)].mean(),2)\n","            df['prob'+str(pred2)] = round(data['prob'+str(pred2)].mean(),2)\n","        else:\n","            pred1Len = len(data[data.prediction==df['prediction']])\n","            pred1 = int(df['prediction'])\n","            pred2 = 0 if pred1 == 1 else 1\n","            total = data['prediction'].count()\n","\n","            df['prob'+str(pred1)] = round(pred1Len/total,2)\n","            df['prob'+str(pred2)] = 1-df['prob'+str(pred1)]\n","\n","        return pd.Series(df, index=['child_id', 'task', 'truth', 'prediction', 'prob0', 'prob1'])\n","\n","    # Aggregating the data\n","    aggTaskMVResDF = aggTaskMVResDF.apply(aggTaskMVData, isProb)\n","    # Drop the multiindex\n","    aggTaskMVResDF.reset_index(drop=True, inplace=True)\n","\n","    return aggTaskMVResDF\n","\n","# Aggregate children's tasks WV\n","def getAggregatedTasksWVResults(resultsDF, isProb):\n","    aggTaskWVResDF = resultsDF.copy()\n","\n","    # Grouping the data based on task\n","    aggTaskWVResDF = aggTaskWVResDF.groupby(['task','child_id'])\n","\n","    def aggTaskWVData(data):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['task'] = data['task'].min()\n","        df['truth'] = data['truth'].min()\n","\n","        if len(data['prediction'].unique()) > 1:\n","            pred1 = int(data['prediction'].sort_values().unique()[0])\n","            pred2 = int(data['prediction'].sort_values().unique()[1])\n","\n","            # Consider only when prediction is correct\n","            #prob1 = data[data['prediction']==pred1]['prob'+str(pred1)].mean()\n","            #prob2 = data[data['prediction']==pred2]['prob'+str(pred2)].mean()\n","\n","            # Add up all probabilities regardless of prediction\n","            prob1 = round(data['prob'+str(pred1)].mean(),2)\n","            prob2 = round(data['prob'+str(pred2)].mean(),2)\n","\n","            df['prediction'] = pred1 if prob1 > prob2 else pred2 # if prob equals, favore the sick prediction\n","            df['prob'+str(pred1)] = prob1\n","            df['prob'+str(pred2)] = prob2\n","        else:\n","            df['prediction'] = data['prediction'].min()\n","\n","            pred1 = int(df['prediction'])\n","            pred2 = 0 if pred1 == 1 else 1\n","\n","            df['prob'+str(pred1)] = round(data['prob'+str(pred1)].mean(),2)\n","            df['prob'+str(pred2)] = round(data['prob'+str(pred2)].mean(),2)\n","\n","        return pd.Series(df, index=['child_id', 'task', 'truth', 'prediction', 'prob0', 'prob1'])\n","\n","    # Aggregating the data\n","    aggTaskWVResDF = aggTaskWVResDF.apply(aggTaskWVData)\n","    # Drop the multiindex\n","    aggTaskWVResDF.reset_index(drop=True, inplace=True)\n","\n","    return aggTaskWVResDF\n","\n","# Aggregate children's tasks Ratio\n","def getAggregatedTasksRatioResults(resultsDF, ratio, isProb):\n","    aggTaskRatioResDF = resultsDF.copy()\n","\n","    # Grouping the data based on task\n","    aggTaskRatioResDF = aggTaskRatioResDF.groupby(['task','child_id'])\n","\n","    def aggTaskRatioData(data, isProb):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['task'] = data['task'].min()\n","        df['truth'] = data['truth'].min()\n","\n","        total = data['prediction'].count()\n","\n","        if len(data['prediction'].unique()) > 1:\n","            pred1 = data['prediction'].sort_values().unique()[0]\n","            count1 = data[data['prediction']==pred1].prediction.count()\n","            prob1 = round(count1/total*100,2)\n","\n","            pred2 = data['prediction'].sort_values().unique()[1]\n","            count2 = data[data['prediction']==pred2].prediction.count()\n","            prob2 = round(count2/total*100,2)\n","\n","            df['prediction'] = pred2 if prob2 >= ratio else pred1 # math.ceil(data['total']*ratio)\n","        else:\n","            df['prediction'] = max(set(data['prediction']), key = data['prediction'].to_list().count)\n","\n","        pred = int(df['prediction'])\n","        pred = 0 if pred == -1 else pred\n","\n","        if isProb:\n","            df['prob'] =  round(data['prob'+str(pred)].min()*100,2)\n","        else :\n","            pred = len(data[data.prediction==df['prediction']])\n","            total = data['prediction'].count()\n","            df['prob'] = round(pred/total*100,2)\n","\n","        df['total'] = data['prediction'].count()\n","\n","        return pd.Series(df, index=['child_id', 'task', 'truth', 'prediction', 'prob', 'total'])\n","\n","    # Aggregating the data\n","    aggTaskRatioResDF = aggTaskRatioResDF.apply(aggTaskRatioData, isProb)\n","    # Drop the multiindex\n","    aggTaskRatioResDF.reset_index(drop=True, inplace=True)\n","\n","    return aggTaskRatioResDF"],"metadata":{"id":"dS9oSXpEw1Nx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aggregate children MV\n","def getAggregatedChildrenResultsMV(resultsDF):\n","    aggChildMajVResultsDF = resultsDF.copy()\n","\n","    # Grouping the data based on children\n","    aggChildMajVResultsDF = aggChildMajVResultsDF.groupby(['child_id'])\n","\n","    def aggChildMajVData(data):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['truth'] = data['truth'].min()\n","        df['prediction'] = get_max_count(data['prediction'].to_list())#, round(data['prob0'].mean(),2), round(data['prob1'].mean(),2))\n","        pred1, pred2 = 0, 1\n","        df['prob'+str(pred1)] = round(data['prob'+str(pred1)].mean(),2)\n","        df['prob'+str(pred2)] = round(data['prob'+str(pred2)].mean(),2)\n","\n","        return pd.Series(df, index=['child_id', 'truth', 'prediction', 'prob0', 'prob1'])\n","\n","    # Aggregating the data\n","    aggChildMajVResultsDF = aggChildMajVResultsDF.apply(aggChildMajVData)\n","    # Drop the multiindex\n","    aggChildMajVResultsDF.reset_index(drop=True, inplace=True)\n","\n","    return aggChildMajVResultsDF\n","\n","# Aggregate children WV\n","def getAggregatedChildrenResultsWV(resultsDF):\n","    # Grouping the data based on children\n","    aggChildWVResultsDF = resultsDF.copy().groupby(['child_id'])\n","\n","    def aggChildWVData(data):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['truth'] = data['truth'].min()\n","\n","        if len(data['prediction'].unique()) > 1:\n","            pred1 = int(data['prediction'].sort_values().unique()[0])\n","            pred2 = int(data['prediction'].sort_values().unique()[1])\n","\n","            # Consider only when prediction is correct\n","            #prob1 = data[data['prediction']==pred1]['prob'+str(pred1)].mean()\n","            #prob2 = data[data['prediction']==pred2]['prob'+str(pred2)].mean()\n","\n","            # Add up all probabilities regardless of prediction\n","            prob1 = round(data['prob'+str(pred1)].mean(),2)\n","            prob2 = round(data['prob'+str(pred2)].mean(),2)\n","\n","            df['prediction'] = pred1 if prob1 > prob2 else pred2 # if prob equals, favore the sick prediction\n","            df['prob'+str(pred1)] = prob1\n","            df['prob'+str(pred2)] = prob2\n","        else:\n","            df['prediction'] = data['prediction'].min()\n","\n","            pred1 = int(df['prediction'])\n","            pred2 = 0 if pred1 == 1 else 1\n","\n","            df['prob'+str(pred1)] = round(data['prob'+str(pred1)].mean(),2)\n","            df['prob'+str(pred2)] = round(data['prob'+str(pred2)].mean(),2)\n","\n","        return pd.Series(df, index=['child_id', 'truth', 'prediction', 'prob0', 'prob1'])\n","\n","    # Aggregating the data # Drop the multiindex\n","    aggChildWVResultsDF = aggChildWVResultsDF.apply(aggChildWVData).reset_index(drop=True)\n","\n","    return aggChildWVResultsDF\n","\n","# Aggregate children ratio\n","def getAggregatedChildrenResultsRatio(resultsDF, ratio):\n","    aggChildRatioResultsDF = resultsDF.copy()\n","\n","    # Grouping the data based on children\n","    aggChildRatioResultsDF = aggChildRatioResultsDF.groupby(['child_id'])\n","\n","    def aggChildRatioData(data):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['truth'] = data['truth'].min()\n","\n","        total = data['prediction'].count()\n","\n","        if len(data['prediction'].unique()) > 1:\n","            pred1 = data['prediction'].sort_values().unique()[0]\n","            count1 = data[data['prediction']==pred1].prediction.count()\n","            prob1 = round(count1/total*100,2)\n","\n","            pred2 = data['prediction'].sort_values().unique()[1]\n","            count2 = data[data['prediction']==pred2].prediction.count()\n","            prob2 = round(count2/total*100,2)\n","\n","            df['prediction'] = pred2 if prob2 >= ratio else pred1 # math.ceil(data['total']*ratio)\n","        else:\n","            df['prediction'] = max(set(data['prediction']), key = data['prediction'].to_list().count)\n","\n","        return pd.Series(df, index=['child_id', 'truth', 'prediction'])\n","    # Aggregating the data\n","    aggChildRatioResultsDF = aggChildRatioResultsDF.apply(aggChildRatioData)\n","    # Drop the multiindex\n","    aggChildRatioResultsDF.reset_index(drop=True, inplace=True)\n","\n","    return aggChildRatioResultsDF\n","\n","# Aggregate children from aggregated tasks (ratio)\n","def getAggregatedTaskChildrenResultsRatioV(aggTaskMVResDF, ratio):\n","    aggChildTaskMajRatioVResultsDF = aggTaskMVResDF.copy()\n","\n","    # Grouping the data based on children\n","    aggChildTaskMajRatioVResultsDF = aggChildTaskMajRatioVResultsDF.groupby(['child_id'])\n","\n","    def aggChildRatioVData(data):\n","        df = {}\n","        df['child_id'] = data['child_id'].min()\n","        df['truth'] = data['truth'].min()\n","\n","        total = data['prediction'].count()\n","\n","        if len(data['prediction'].unique()) > 1:\n","            pred1 = data['prediction'].sort_values().unique()[0]\n","            count1 = data[data['prediction']==pred1].prediction.count()\n","            prob1 = round(count1/total*100,2)\n","\n","            pred2 = data['prediction'].sort_values().unique()[1]\n","            count2 = data[data['prediction']==pred2].prediction.count()\n","            prob2 = round(count2/total*100,2)\n","\n","            df['prediction'] = pred2 if prob2 >= ratio else pred1 # math.ceil(data['total']*ratio)\n","        else:\n","            df['prediction'] = max(set(data['prediction']), key = data['prediction'].to_list().count)\n","\n","        return pd.Series(df, index=['child_id', 'truth', 'prediction'])\n","    # Aggregating the data\n","    aggChildTaskMajRatioVResultsDF = aggChildTaskMajRatioVResultsDF.apply(aggChildRatioVData)\n","    # Drop the multiindex\n","    aggChildTaskMajRatioVResultsDF.reset_index(drop=True, inplace=True)\n","\n","    return aggChildTaskMajRatioVResultsDF"],"metadata":{"id":"_qPlGJD2vOfv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calSResults(resultsDF, isProb=True):\n","    repResults = pd.DataFrame(columns=['model', 'task','acc','prec','rec','f1'])\n","\n","#     resultsDF.truth = resultsDF.truth.astype(int)\n","#     resultsDF.prediction = resultsDF.prediction.astype(int)\n","\n","    aggTaskMVResDF = getAggregatedTasksMVResults(resultsDF, isProb)\n","\n","    # Story level\n","    for task in aggTaskMVResDF.task.unique():\n","        repResults.loc[len(repResults)] = ['STMV', task,\n","            round(accuracy_score(aggTaskMVResDF[aggTaskMVResDF.task==task].truth, aggTaskMVResDF[aggTaskMVResDF.task==task].prediction)*100,1),\n","            round(metrics.precision_score(aggTaskMVResDF[aggTaskMVResDF.task==task].truth, aggTaskMVResDF[aggTaskMVResDF.task==task].prediction, average='binary', pos_label=1)*100,1),\n","            round(metrics.recall_score(aggTaskMVResDF[aggTaskMVResDF.task==task].truth, aggTaskMVResDF[aggTaskMVResDF.task==task].prediction, average='binary', pos_label=1)*100,1),\n","            round(metrics.f1_score(aggTaskMVResDF[aggTaskMVResDF.task==task].truth, aggTaskMVResDF[aggTaskMVResDF.task==task].prediction, average='binary', pos_label=1)*100,1)]\n","\n","    # Story agg -> child agg\n","\n","    # Child MV\n","    aggChildMajVResultsDF = getAggregatedChildrenResultsMV(aggTaskMVResDF)\n","    repResults.loc[len(repResults)] = ['STMVCMV', 0,\n","          round(accuracy_score(aggChildMajVResultsDF.truth, aggChildMajVResultsDF.prediction)*100,1),\n","          round(metrics.precision_score(aggChildMajVResultsDF.truth, aggChildMajVResultsDF.prediction, average='binary', pos_label=1)*100,1),\n","          round(metrics.recall_score(aggChildMajVResultsDF.truth, aggChildMajVResultsDF.prediction, average='binary', pos_label=1)*100,1),\n","          round(metrics.f1_score(aggChildMajVResultsDF.truth, aggChildMajVResultsDF.prediction, average='binary', pos_label=1)*100,1)]\n","\n","    # Child WV\n","    aggChildTaskMajWResultsDF = getAggregatedChildrenResultsWV(aggTaskMVResDF)\n","    repResults.loc[len(repResults)] = ['STMVCWV', 0,\n","          round(accuracy_score(aggChildTaskMajWResultsDF.truth, aggChildTaskMajWResultsDF.prediction)*100,1),\n","          round(metrics.precision_score(aggChildTaskMajWResultsDF.truth, aggChildTaskMajWResultsDF.prediction, average='binary', pos_label=1)*100,1),\n","          round(metrics.recall_score(aggChildTaskMajWResultsDF.truth, aggChildTaskMajWResultsDF.prediction, average='binary', pos_label=1)*100,1),\n","          round(metrics.f1_score(aggChildTaskMajWResultsDF.truth, aggChildTaskMajWResultsDF.prediction, average='binary', pos_label=1)*100,1)]\n","\n","    return repResults\n"],"metadata":{"id":"zJFwz121w6Jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Architecture and Data Pre-Processing"],"metadata":{"id":"UABcjee4w9vt"}},{"cell_type":"code","source":["# Used during training by the optimiser function\n","\n","def keras_f1_score(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","\n","    return f1_val"],"metadata":{"id":"VarfYXrhw6MH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the model's architecture\n","\n","def attachment_model(sequence_length, num_columns, n_classes, lstm_units, att_units, att_type, att_window, att_hist, att_act, dense_units):\n","\n","    # Clearn the session\n","    tf.keras.backend.clear_session()\n","\n","    # Optimiser\n","    opt = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n","\n","    # Layers\n","    model = Sequential()\n","    # Masking to ignore the padding\n","    model.add(Masking(mask_value=0., input_shape=(sequence_length, num_columns)))\n","    # LSTM\n","    model.add(LSTM(lstm_units, seed=seed_value, return_sequences=True))\n","    # Attention to enhance the recall\n","    model.add(SeqSelfAttention(units=att_units,\n","            attention_type=att_type,\n","            kernel_initializer=keras.initializers.GlorotNormal(seed=seed_value),\n","            kernel_regularizer=keras.regularizers.l2(1e-4),\n","            bias_regularizer=keras.regularizers.l1(1e-4),\n","            attention_regularizer_weight=1e-4,\n","            attention_width=att_window,\n","            history_only=att_hist,\n","            attention_activation=att_act,\n","            name='Attention'))\n","    # trnasforms 3D output to 2D\n","    model.add(Flatten())\n","    model.add(Dense(dense_units, activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(n_classes, activation='sigmoid')) #'linear'\n","\n","    # Compiling the model, optimise the F1-score\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[keras_f1_score])\n","\n","    return model"],"metadata":{"id":"PRFykAFhw6PF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To play with the information written in the log files\n","class CustomCSVLogger(tf.keras.callbacks.CSVLogger):\n","    def __init__(self, cusParams, filename, separator=',', append=False):\n","        super().__init__(filename, separator, append)\n","\n","        self.cusParams = cusParams\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # You can access the logs dictionary which contains metrics like loss and accuracy\n","        # Add your custom information to logs dictionary\n","        for k, v in self.cusParams.items():\n","            logs[k] = v\n","\n","        # Call the parent class's on_epoch_end method to log the standard information\n","        super().on_epoch_end(epoch, logs)"],"metadata":{"id":"-I1VKSkIF995"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process the data to be used by the model\n","\n","# each person's data was segmented, the code reads a person's data, put them in one variable, and pads it to match the length of the max data\n","\n","class ReadEmoWeightsDataset:\n","    def __init__(self, data_folder, df, data_indices, batch_size=32):\n","        # stores information about the data location\n","        self.data_folder = data_folder\n","        self.file_list = os.listdir(data_folder)\n","        self.dataDF = df\n","\n","        # learns the max length to be used for padding (all data are padded to this length)\n","        max_seq_length = 0\n","        for file in self.file_list:\n","            seg = int(file.split('_')[1].split('.')[0])\n","            max_seq_length = seg if seg > max_seq_length else max_seq_length\n","\n","        self.max_seq_length = max_seq_length\n","        self.children_ids = data_indices\n","        self.batch_size = batch_size # used for training\n","\n","    # Custom key function to extract the numeric part before '_'\n","    # this refers to the segment number, it is needed to be able to maintaine the order of the data segments\n","    def key_func(self, s):\n","        # if ind == 0:\n","        #     return int(s.split('_')[ind])\n","        # elif ind == 1:\n","        return int(s.split('_')[1].split('.')[0])\n","\n","    def collect_data(self, idx): # works on each person separately, the idx used to index the people\n","        # select one person\n","        child_id = list(self.children_ids)[idx]\n","\n","        # Find all files belonging to the same person\n","        children_files = [file_name for file_name in self.file_list if file_name.startswith(str(child_id)+'_')]\n","        # Sort the data to maintain the order of segments\n","        sorted_children_files = sorted(children_files, key=lambda x: self.key_func(x))\n","\n","        # Read data from files and combine into array of arrays\n","        array = []\n","        for file_name in sorted_children_files:\n","            file_path = os.path.join(self.data_folder, file_name)\n","            child_data = np.load(file_path)['arr_0'][0]\n","            array.append(child_data)\n","        array = np.array(array)\n","\n","        # Calculate the number of rows to pad (how may rows I need to add to match the lengths)\n","        rows_to_pad = max(0, self.max_seq_length - array.shape[0])\n","\n","        # Pad the array along the first dimension\n","        padding = np.zeros((rows_to_pad, *array.shape[1:]), dtype=array.dtype)\n","        padded_array = np.concatenate((array, padding), axis=0)\n","\n","        combined_array = np.array(padded_array)\n","\n","        # Get label from the data dataframe\n","        label = int(self.dataDF[self.dataDF.childID==int(child_id)].insecure.values[0])\n","\n","        return combined_array, label\n","\n","\n","    def __len__(self):\n","        return len(self.children_ids)\n","\n","    def __iter__(self):\n","        self.index = 0\n","        return self\n","\n","    def __next__(self):\n","        # used by the batch function of the model, to select BATCH number of children\n","        if self.index >= len(self):\n","            raise StopIteration\n","\n","        batch_data = []\n","        batch_labels = []\n","\n","        for idx in range(self.index, min(self.index + self.batch_size, len(self))):\n","            combined_array, label = self.collect_data(idx)\n","\n","            batch_data.append(combined_array)\n","            batch_labels.append(label)\n","\n","        self.index += self.batch_size\n","\n","        return np.array(batch_data), np.array(batch_labels)\n","\n","    def __getitem__(self, idx):\n","        if idx >= len(self):\n","            raise IndexError(\"Index out of range\")\n","\n","        batch_data = []\n","        batch_labels = []\n","\n","        combined_array, label = self.collect_data(idx)\n","\n","        batch_data.append(combined_array)\n","        batch_labels.append(label)\n","\n","        return np.array(batch_data), np.array(batch_labels)\n"],"metadata":{"id":"RVWLtMrTBiBB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-Tune Model"],"metadata":{"id":"jj-JEimTxHhZ"}},{"cell_type":"markdown","source":["## Parameters"],"metadata":{"id":"xy_zeRJy3WPa"}},{"cell_type":"code","source":["class cConfigs:\n","    def __init__(self, lstm_units, dense_units, att_window, att_units, att_type, att_hist, att_act,\n","                 batch_size=4, epochs=20, dropout=0.2, lr=0.001):\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","\n","        self.lstm_units = lstm_units\n","        self.dense_units = dense_units # 64\n","        self.dropout = dropout # , 0.4\n","        self.lr = lr # , 0.0001\n","\n","        self.att_units = att_units # 32\n","        self.att_type = att_type #, SeqSelfAttention.ATTENTION_TYPE_ADD]\n","        self.att_window = att_window\n","        self.att_hist =  att_hist #, False]\n","        self.att_act = att_act # None,\n","\n","        self.params = {'batch_size': batch_size, 'epochs': epochs, 'lstm_units':lstm_units,\n","                       'dense_units': dense_units, 'dropout': dropout, 'lr': lr,\n","                       'att_units': att_units, 'att_type': att_type, 'att_window': att_window,\n","                       'att_hist': att_hist, 'att_act': att_act}\n","\n","    def set_param(self, newParams):\n","        for k, v in newParams.items():\n","            self.params[k] = v\n","\n","    def get_params(self):\n","        return self.params\n","\n","    def get_tuned_params(self):\n","        tuned = {}\n","\n","        for k, v in self.params.items():\n","            if type(v) == list:\n","                tuned[k] = v\n","        return tuned\n","\n","    def get_nontuned_params(self):\n","        tuned = {}\n","\n","        for k, v in self.params.items():\n","            if type(v) != list:\n","                tuned[k] = v\n","        return tuned"],"metadata":{"id":"L3pxe__Rw6Rl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create All combinations\n","\n","def getCombinations(obj, n_nested_levels):\n","    curLevelArr = []\n","\n","    # If this is the last nested level, return the elemnts as individual arrays\n","    if n_nested_levels == 0:\n","        for item in obj[0]:\n","            curLevelArr.append([item])\n","\n","        return curLevelArr\n","\n","    else:\n","        # Call the function again\n","        subLevelArr = getCombinations(obj[1:], n_nested_levels-1)\n","\n","        # Add the current level items to the existing sublists\n","        for subList in subLevelArr:\n","            for element in obj[0]: # current level of the nested structure\n","                curLevelArr.append([element]+subList)\n","\n","    return curLevelArr"],"metadata":{"id":"2qUZ_gIcw6UB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type1configs = cConfigs(lstm_units=[16, 32, 64], dense_units=[8, 16, 32], att_window=[2, 4, 8],\n","                        att_units=[None], att_type=['multiplicative'],\n","                        att_hist=[True, False], att_act=[None, 'Softmax'])\n","\n","type2configs = cConfigs(lstm_units=[16, 32, 64], dense_units=[8, 16, 32], att_window=[2, 4, 8],\n","                   att_units=[8, 16, 32], att_type=['additive'],\n","                   att_hist=[True, False], att_act=[None, 'Softmax'])\n","\n","selParams1 = type1configs.get_tuned_params()\n","selParams2 = type2configs.get_tuned_params()"],"metadata":{"id":"NuFaLNc3xU6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["possibleConfigs1 = []\n","params2tune1 = list(selParams1.values())\n","possibleConfigs1 = getCombinations(params2tune1, len(params2tune1)-1)\n","\n","possibleConfigs2 = []\n","params2tune2 = list(selParams2.values())\n","possibleConfigs2 = getCombinations(params2tune2, len(params2tune2)-1)"],"metadata":{"id":"aTisev0zxU9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(possibleConfigs1), len(possibleConfigs2), len(possibleConfigs1) + len(possibleConfigs2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxQtGf3QxX0z","executionInfo":{"status":"ok","timestamp":1712169674023,"user_tz":-180,"elapsed":12,"user":{"displayName":"Lab SocialSP","userId":"02146407728601162358"}},"outputId":"957d376b-e7e2-4b08-c3d9-6de4598e18c4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(108, 324, 432)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["allPossibleConfigs = possibleConfigs1 + possibleConfigs2\n","\n","# Select only 20 possible sets\n","selectedConfigs = random.sample(allPossibleConfigs, 20) #### 1/3rd of possiple combinations"],"metadata":{"id":"OIo-0LJqxX4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selectedConfigs # 1, 10, 11, 18"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftVkdDtyxU_c","executionInfo":{"status":"ok","timestamp":1712169674023,"user_tz":-180,"elapsed":6,"user":{"displayName":"Lab SocialSP","userId":"02146407728601162358"}},"outputId":"0d5af664-f027-4e6c-f580-cb974e101e2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[16, 16, 8, 'additive', 8, True, 'Softmax'],\n"," [16, 16, None, 'multiplicative', 2, True, 'Softmax'],\n"," [16, 16, None, 'multiplicative', 4, True, None],\n"," [32, 8, 8, 'additive', 4, False, 'Softmax'],\n"," [64, 16, 8, 'additive', 4, True, None],\n"," [64, 32, 16, 'additive', 2, True, None],\n"," [16, 32, 8, 'additive', 2, True, None],\n"," [64, 32, None, 'multiplicative', 4, True, 'Softmax'],\n"," [64, 32, 32, 'additive', 2, False, 'Softmax'],\n"," [32, 32, None, 'multiplicative', 8, False, None],\n"," [32, 16, 32, 'additive', 8, True, 'Softmax'],\n"," [16, 8, 16, 'additive', 2, True, 'Softmax'],\n"," [64, 32, None, 'multiplicative', 4, False, None],\n"," [64, 16, 8, 'additive', 4, True, 'Softmax'],\n"," [16, 8, 8, 'additive', 4, False, None],\n"," [32, 32, None, 'multiplicative', 4, True, None],\n"," [16, 32, None, 'multiplicative', 4, True, None],\n"," [64, 8, None, 'multiplicative', 8, False, None],\n"," [16, 16, 8, 'additive', 2, True, None],\n"," [64, 8, 16, 'additive', 2, True, None]]"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## Code"],"metadata":{"id":"ABZCB6_2xLmd"}},{"cell_type":"code","source":["# Nested CV\n","n_outer_folds = 4 # number of folds\n","n_inner_folds = 3\n","random_state = 42\n","\n","n_classes = 1 # one label (0,1)\n","num_columns = 1024 # the columns from the wav2vec2 model\n","\n","resultColumns = ['truth', 'prediction', 'prob0', 'prob1', 'child_id', 'task', 'ofold', 'ifold', 'run', 'source', 'elapsedtime', 'experiment']\n","\n","save_path = path+'Audio/Finetuned/'\n","save_res_path = path+'Audio/'\n","\n","for experiment, choices in enumerate(tqdm(selectedConfigs, desc=\" exp\", position=0, leave=True)): # TODO\n","    experiment += 1 # TODO\n","\n","    expParams = type1configs.get_nontuned_params() # get the parameters that will be used for this model\n","\n","    for k, v in zip(list(selParams1.keys()),choices):\n","        expParams[k] = v\n","\n","    model_path = save_path+str(experiment)+'/'\n","\n","    # Create a folder for this experiment\n","    if str(experiment) not in os.listdir(save_path): os.makedirs(model_path)\n","\n","    for run in tqdm(range(1, 4), desc=\" runs\", position=1, leave=False): # 3 runs # TODO\n","        start_time = time.time()\n","\n","        cp_path = model_path+str(run)+'/'\n","        # Create folders\n","        if str(run) not in os.listdir(model_path): os.makedirs(cp_path)\n","\n","        # Go through tasks\n","        for task in tqdm(data.task.unique(), desc=\" tasks\", position=2, leave=False): # TODO\n","            start_time = time.time()\n","\n","            resultsTr = []\n","            resultsVal = []\n","            resultsTs = []\n","\n","            # Get task/story data\n","            workingData = data[data.task == task].sort_values('childID').reset_index(drop = True)\n","\n","            sequence_length = workingData.groupby(['childID']).count().task.max() # use any column, there are the same\n","\n","            # to be used by the cross-validation function\n","            childIDs = workingData.childID.unique() # get the ids\n","            childLabels = workingData.groupby('childID').first().insecure.values # get the labels\n","\n","            data_folder = path+'Audio/Data/Weights/{}/'.format(task)\n","\n","            # Define the outer kfold\n","            kfold_test = StratifiedKFold(n_outer_folds, shuffle=True, random_state=random_state)\n","\n","            for ofold, (train_indices, test_indices) in enumerate(tqdm(kfold_test.split(childIDs, childLabels), total=n_outer_folds, desc=\" ofolds\", position=3, leave=False)):\n","                # Get the train data, to be split 2 train and 1 validation\n","                innerChildIDs = workingData[workingData.childID.isin(childIDs[train_indices])].childID.unique()\n","                innerChildLabels = workingData[workingData.childID.isin(childIDs[train_indices])].groupby('childID').first().insecure.values\n","\n","                # Define the inner fold\n","                kfold_val = StratifiedKFold(n_inner_folds, shuffle=True, random_state=random_state)\n","\n","                for ifold, (inner_train_indices, val_indices) in enumerate(tqdm(kfold_val.split(innerChildIDs, innerChildLabels), total=n_inner_folds, desc=\" ifolds\", position=4, leave=False)):\n","                    # Prepare the data, use the custom class to read and pad the data\n","\n","                    # Inner (cross-validation) train set\n","                    dataset_train = ReadEmoWeightsDataset(data_folder, workingData, innerChildIDs[inner_train_indices], batch_size=len(innerChildIDs[inner_train_indices]))\n","                    # Use tf.data.Dataset to create a dataset from the data, to simplify the training process\n","                    tf_dataset_train = tf.data.Dataset.from_generator(dataset_train.__iter__, (tf.float32, tf.int32), ((None, None, None), (None,)))\n","                    x_train, y_train = next(iter(tf_dataset_train))\n","\n","                    # Val set\n","                    dataset_val = ReadEmoWeightsDataset(data_folder, workingData, innerChildIDs[val_indices], batch_size=len(innerChildIDs[val_indices]))\n","                    # Use tf.data.Dataset to create a dataset from the data, to simplify the training process\n","                    tf_dataset_val = tf.data.Dataset.from_generator(dataset_val.__iter__, (tf.float32, tf.int32), ((None, None, None), (None,)))\n","                    x_val, y_val = next(iter(tf_dataset_val))\n","\n","                    # Use the model from the Model Architecture section\n","                    model = attachment_model(sequence_length, num_columns, n_classes, lstm_units=expParams['lstm_units'],\n","                                             att_window=expParams['att_window'], dense_units=expParams['dense_units'], att_units=expParams['att_units'],\n","                                             att_type=expParams['att_type'], att_hist=expParams['att_hist'], att_act=expParams['att_act'])\n","\n","                    # A callback that saves the model's weights\n","                    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=cp_path, save_weights_only=True, verbose=0,\n","                                                                    monitor=\"val_keras_f1_score\", mode=\"max\", save_best_only=True)\n","\n","                    expParams['task'], expParams['ofold'], expParams['ifold'] = task, ofold, ifold\n","\n","\n","                    # Logging performacen callback\n","                    history_callback = CustomCSVLogger(expParams, cp_path+'log.csv', separator=\",\", append=True)\n","\n","                    # Early stopping callback\n","                    estopping_callback = keras.callbacks.EarlyStopping(monitor=\"val_keras_f1_score\", patience=3, min_delta=0.001, mode=\"max\", restore_best_weights=False)\n","\n","                    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), verbose=0,\n","                                        epochs=expParams['epochs'], batch_size=expParams['batch_size'],\n","                                        callbacks=[history_callback, estopping_callback, cp_callback])\n","                    # Retrieve best model\n","                    # The model weights (that are considered the best)\n","                    model.load_weights(cp_path)\n","\n","                    # Evaluation\n","                    pred_inner_train = model(x_train)\n","                    pred_val = model(x_val)\n","\n","                    end_time = time.time()\n","\n","                    # Save results\n","                    resultsTr.append([y_train, pred_inner_train, innerChildIDs[inner_train_indices], task, ofold, ifold, run, end_time - start_time, experiment])\n","                    resultsVal.append([y_val, pred_val, innerChildIDs[val_indices], task, ofold, ifold, run, end_time - start_time, experiment])\n","\n","\n","            # Store the data in a CSV file\n","            resultsDF = pd.DataFrame(columns=resultColumns)\n","\n","            for row in resultsTr:\n","                resultTempDF = pd.DataFrame()\n","                resultTempDF[resultColumns[0]] = row[0]#.values\n","                resultTempDF[resultColumns[1]] = [0 if prob1.numpy().tolist()[0] < 0.5 else 1 for prob1 in row[1]]\n","                resultTempDF[resultColumns[2]] = [1 - prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[3]] = [prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[4]] = row[2]\n","                resultTempDF[resultColumns[5]] = row[3]\n","                resultTempDF[resultColumns[6]] = row[4]\n","                resultTempDF[resultColumns[7]] = row[5]\n","                resultTempDF[resultColumns[8]] = row[6]\n","                resultTempDF[resultColumns[9]] = 'train'\n","                resultTempDF[resultColumns[10]] = row[7]\n","                resultTempDF[resultColumns[11]] = row[8]\n","                resultsDF = pd.concat([resultsDF, resultTempDF], axis=0)\n","\n","            for row in resultsVal:\n","                resultTempDF = pd.DataFrame()\n","                resultTempDF[resultColumns[0]] = row[0]#.values\n","                resultTempDF[resultColumns[1]] = [0 if prob1.numpy().tolist()[0] < 0.5 else 1 for prob1 in row[1]]\n","                resultTempDF[resultColumns[2]] = [1 - prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[3]] = [prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[4]] = row[2]\n","                resultTempDF[resultColumns[5]] = row[3]\n","                resultTempDF[resultColumns[6]] = row[4]\n","                resultTempDF[resultColumns[7]] = row[5]\n","                resultTempDF[resultColumns[8]] = row[6]\n","                resultTempDF[resultColumns[9]] = 'val'\n","                resultTempDF[resultColumns[10]] = row[7]\n","                resultTempDF[resultColumns[11]] = row[8]\n","                resultsDF = pd.concat([resultsDF, resultTempDF], axis=0)\n","\n","            resultsDF.reset_index(inplace=True, drop=True)\n","            #resultsDF.to_csv('ResultsNNLIWC/speech_results_liwc_cnn_v5_e20_runs'+str(run)+'_v13.csv',index=False)\n","\n","            with open(save_path+'Results/audio_emo_e_'+str(experiment)+'.csv', 'a') as f1:\n","                resultsDF.to_csv(f1, mode='a', index=False, header=f1.tell()==0)\n","\n"],"metadata":{"id":"MY15CEf_xNAS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Final Model"],"metadata":{"id":"Hjs89NWI3ZtO"}},{"cell_type":"code","source":["# Nested CV\n","n_outer_folds = 4\n","n_inner_folds = 3\n","random_state = 42\n","\n","n_classes = 1\n","num_columns = 1024\n","\n","resultColumns = ['truth', 'prediction', 'prob0', 'prob1', 'child_id', 'task', 'ofold', 'run', 'source', 'elapsedtime', 'experiment']\n","\n","save_path = path+'Audio/Final/'\n","save_res_path = path+'Audio/'\n","\n","# I am running one model only, the one with the parameters I selected from selectedConfigs\n","for experiment, choices in enumerate(tqdm(selectedConfigs[13:14], desc=\" exp\", position=0, leave=True)): # TODO\n","    experiment += 14 # TODO: this is just the number of expirement, used to create a folder for this run's data\n","\n","    expParams = type1configs.get_nontuned_params()\n","\n","    for k, v in zip(list(selParams1.keys()),choices):\n","        expParams[k] = v\n","\n","    model_path = save_path+str(experiment)+'/'\n","    # Create folders\n","    if str(experiment) not in os.listdir(save_path): os.makedirs(model_path)\n","\n","    for run in tqdm(range(1, 11), desc=\" runs\", position=1, leave=False): # 10 runs # TODO\n","        start_time = time.time()\n","\n","        cp_path = model_path+str(run)+'/'\n","        # Create folders\n","        if str(run) not in os.listdir(model_path): os.makedirs(cp_path)\n","\n","        # Go through tasks\n","        for task in tqdm(data.task.unique(), desc=\" tasks\", position=2, leave=False): # TODO\n","            start_time = time.time()\n","\n","            resultsTr = []\n","            resultsVal = []\n","            resultsTs = []\n","\n","            # Get task data\n","            workingData = data[data.task == task].sort_values('childID').reset_index(drop = True)\n","\n","            sequence_length = workingData.groupby(['childID']).count().task.max() # use any column\n","\n","            childIDs = workingData.childID.unique()\n","            childLabels = workingData.groupby('childID').first().insecure.values\n","\n","            data_folder = path+'Audio/Data/Weights/{}/'.format(task)\n","\n","            # Define the outer kfold\n","            kfold_test = StratifiedKFold(n_outer_folds, shuffle=True, random_state=random_state)\n","\n","            for ofold, (train_indices, test_indices) in enumerate(tqdm(kfold_test.split(childIDs, childLabels), total=n_outer_folds, desc=\" ofolds\", position=3, leave=False)):\n","                # Define the inner fold\n","                kfold_val = StratifiedKFold(n_inner_folds, shuffle=True, random_state=random_state)\n","\n","                # Prepare the train set, use the custom class to read and pad the data\n","                dataset_train = ReadEmoWeightsDataset(data_folder, workingData, childIDs[train_indices], batch_size=len(childIDs[train_indices]))\n","                # Use tf.data.Dataset to create a dataset from the data, to simplify the training process\n","                tf_dataset_train = tf.data.Dataset.from_generator(dataset_train.__iter__, (tf.float32, tf.int32), ((None, None, None), (None,)))\n","                x_train, y_train = next(iter(tf_dataset_train))\n","\n","                # Prepare the test set, use the custom class to read and pad the data\n","                dataset_test = ReadEmoWeightsDataset(data_folder, workingData, childIDs[test_indices], batch_size=len(childIDs[test_indices]))\n","                # Use tf.data.Dataset to create a dataset from the data, to simplify the training process\n","                tf_dataset_test = tf.data.Dataset.from_generator(dataset_test.__iter__, (tf.float32, tf.int32), ((None, None, None), (None,)))\n","                x_test, y_test = next(iter(tf_dataset_test))\n","\n","                # Use the model from the Model Architecture section\n","                model = attachment_model(sequence_length, num_columns, n_classes, lstm_units=expParams['lstm_units'],\n","                                         att_window=expParams['att_window'], dense_units=expParams['dense_units'], att_units=expParams['att_units'],\n","                                         att_type=expParams['att_type'], att_hist=expParams['att_hist'], att_act=expParams['att_act'])\n","\n","                # A callback that saves the model's weights\n","                cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=cp_path, save_weights_only=True, verbose=0,\n","                                                                monitor=\"keras_f1_score\", mode=\"max\", save_best_only=True)\n","\n","                expParams['task'], expParams['ofold'] = task, ofold\n","\n","\n","                # Logging performacen callback\n","                history_callback = CustomCSVLogger(expParams, cp_path+'log.csv', separator=\",\", append=True)\n","\n","                # Early stopping callback\n","                estopping_callback = keras.callbacks.EarlyStopping(monitor=\"keras_f1_score\", patience=3, min_delta=0.001, mode=\"max\", restore_best_weights=False)\n","\n","                history = model.fit(x_train, y_train, verbose=0,\n","                                   epochs=expParams['epochs'], batch_size=expParams['batch_size'],\n","                                   callbacks=[history_callback, estopping_callback, cp_callback])\n","\n","                # Retrieve best model\n","                # The model weights (that are considered the best)\n","                model.load_weights(cp_path)\n","\n","                # Evaluation\n","                pred_inner_train = model(x_train)\n","                pred_test = model(x_test)\n","\n","                end_time = time.time()\n","\n","                # Save results\n","                resultsTr.append([y_train, pred_inner_train, childIDs[train_indices], task, ofold, run, end_time - start_time, experiment])\n","                resultsTs.append([y_test, pred_test, childIDs[test_indices], task, ofold, run, end_time - start_time, experiment])\n","\n","\n","            # Store the data in CSV files\n","            resultsDF = pd.DataFrame(columns=resultColumns)\n","\n","            for row in resultsTr:\n","                resultTempDF = pd.DataFrame()\n","                resultTempDF[resultColumns[0]] = row[0]#.values\n","                resultTempDF[resultColumns[1]] = [0 if prob1.numpy().tolist()[0] < 0.5 else 1 for prob1 in row[1]]\n","                resultTempDF[resultColumns[2]] = [1 - prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[3]] = [prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[4]] = row[2]\n","                resultTempDF[resultColumns[5]] = row[3]\n","                resultTempDF[resultColumns[6]] = row[4]\n","                resultTempDF[resultColumns[7]] = row[5]\n","                resultTempDF[resultColumns[8]] = 'train'\n","                resultTempDF[resultColumns[9]] = row[6]\n","                resultTempDF[resultColumns[10]] = row[7]\n","                resultsDF = pd.concat([resultsDF, resultTempDF], axis=0)\n","\n","            for row in resultsTs:\n","                resultTempDF = pd.DataFrame()\n","                resultTempDF[resultColumns[0]] = row[0]#.values\n","                resultTempDF[resultColumns[1]] = [0 if prob1.numpy().tolist()[0] < 0.5 else 1 for prob1 in row[1]]\n","                resultTempDF[resultColumns[2]] = [1 - prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[3]] = [prob1.numpy().tolist()[0] for prob1 in row[1]]\n","                resultTempDF[resultColumns[4]] = row[2]\n","                resultTempDF[resultColumns[5]] = row[3]\n","                resultTempDF[resultColumns[6]] = row[4]\n","                resultTempDF[resultColumns[7]] = row[5]\n","                resultTempDF[resultColumns[8]] = 'test'\n","                resultTempDF[resultColumns[9]] = row[6]\n","                resultTempDF[resultColumns[10]] = row[7]\n","                resultsDF = pd.concat([resultsDF, resultTempDF], axis=0)\n","\n","            resultsDF.reset_index(inplace=True, drop=True)\n","\n","            with open(save_res_path+'Results/audio_emo_e_'+str(experiment)+'.csv', 'a') as f1:\n","                resultsDF.to_csv(f1, mode='a', index=False, header=f1.tell()==0)\n"],"metadata":{"id":"d-UQOwboqfvy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Results"],"metadata":{"id":"933atyA2a-dM"}},{"cell_type":"code","source":["resPath = 'Audio/Results/'\n","experiment = 14 # which experiment to read\n","resDF = pd.read_csv(path+resPath+'audio_emo_e_'+str(experiment)+'.csv', header=0) # collect predictions\n","resTestDF = resDF[resDF.source=='test'] # use the test data only"],"metadata":{"id":"yEbR0NQiYFEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["runResDF = pd.DataFrame()\n","\n","for run in resTestDF.run.unique():\n","    tempDF = calSResults(resTestDF[resTestDF.run==run]) # calculate performance\n","    tempDF['run'] = run\n","    runResDF = pd.concat([runResDF,tempDF], axis=0)\n","\n","runResDF.groupby(['model','task']).agg(['mean','std']).round(2).reset_index(\n",").drop('run', axis=1, level=0).sort_values('task')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"lDeJlZif_rS6","executionInfo":{"status":"ok","timestamp":1712177911179,"user_tz":-180,"elapsed":9260,"user":{"displayName":"Lab SocialSP","userId":"02146407728601162358"}},"outputId":"38347475-6b11-4ba9-c40f-2a7d8c6a1cd5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     model task    acc         prec          rec           f1      \n","                  mean   std   mean   std   mean   std   mean   std\n","5  STMVCMV  0.0  72.69  1.77  69.78  2.37  65.11  1.86  67.37  2.00\n","6  STMVCWV  0.0  72.21  1.94  70.07  2.44  62.44  3.83  66.01  2.86\n","0     STMV  1.0  67.40  1.65  65.46  2.13  51.16  4.91  57.34  3.61\n","1     STMV  2.0  70.78  2.28  66.91  3.31  64.67  4.12  65.65  2.66\n","2     STMV  3.0  65.20  1.18  59.06  1.13  62.94  3.39  60.92  2.01\n","3     STMV  4.0  73.30  1.97  74.03  3.40  57.94  2.46  64.97  2.34\n","4     STMV  5.0  67.18  1.72  63.68  2.94  54.29  1.27  58.59  1.59"],"text/html":["\n","  <div id=\"df-e1ef003e-7da1-408f-aa6e-9462f057e25c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th>model</th>\n","      <th>task</th>\n","      <th colspan=\"2\" halign=\"left\">acc</th>\n","      <th colspan=\"2\" halign=\"left\">prec</th>\n","      <th colspan=\"2\" halign=\"left\">rec</th>\n","      <th colspan=\"2\" halign=\"left\">f1</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>mean</th>\n","      <th>std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>STMVCMV</td>\n","      <td>0.0</td>\n","      <td>72.69</td>\n","      <td>1.77</td>\n","      <td>69.78</td>\n","      <td>2.37</td>\n","      <td>65.11</td>\n","      <td>1.86</td>\n","      <td>67.37</td>\n","      <td>2.00</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>STMVCWV</td>\n","      <td>0.0</td>\n","      <td>72.21</td>\n","      <td>1.94</td>\n","      <td>70.07</td>\n","      <td>2.44</td>\n","      <td>62.44</td>\n","      <td>3.83</td>\n","      <td>66.01</td>\n","      <td>2.86</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>STMV</td>\n","      <td>1.0</td>\n","      <td>67.40</td>\n","      <td>1.65</td>\n","      <td>65.46</td>\n","      <td>2.13</td>\n","      <td>51.16</td>\n","      <td>4.91</td>\n","      <td>57.34</td>\n","      <td>3.61</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>STMV</td>\n","      <td>2.0</td>\n","      <td>70.78</td>\n","      <td>2.28</td>\n","      <td>66.91</td>\n","      <td>3.31</td>\n","      <td>64.67</td>\n","      <td>4.12</td>\n","      <td>65.65</td>\n","      <td>2.66</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>STMV</td>\n","      <td>3.0</td>\n","      <td>65.20</td>\n","      <td>1.18</td>\n","      <td>59.06</td>\n","      <td>1.13</td>\n","      <td>62.94</td>\n","      <td>3.39</td>\n","      <td>60.92</td>\n","      <td>2.01</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>STMV</td>\n","      <td>4.0</td>\n","      <td>73.30</td>\n","      <td>1.97</td>\n","      <td>74.03</td>\n","      <td>3.40</td>\n","      <td>57.94</td>\n","      <td>2.46</td>\n","      <td>64.97</td>\n","      <td>2.34</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>STMV</td>\n","      <td>5.0</td>\n","      <td>67.18</td>\n","      <td>1.72</td>\n","      <td>63.68</td>\n","      <td>2.94</td>\n","      <td>54.29</td>\n","      <td>1.27</td>\n","      <td>58.59</td>\n","      <td>1.59</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1ef003e-7da1-408f-aa6e-9462f057e25c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e1ef003e-7da1-408f-aa6e-9462f057e25c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e1ef003e-7da1-408f-aa6e-9462f057e25c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7da385d0-5663-449f-9b82-c387554cc42d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7da385d0-5663-449f-9b82-c387554cc42d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7da385d0-5663-449f-9b82-c387554cc42d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \")\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": [\n        \"model\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"STMVCMV\",\n          \"STMVCWV\",\n          \"STMV\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"task\",\n        \"\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.9518001458970662,\n        \"min\": 0.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0,\n          1.0,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"acc\",\n        \"mean\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.192474036885884,\n        \"min\": 65.2,\n        \"max\": 73.3,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          72.69,\n          72.21,\n          73.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"acc\",\n        \"std\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.33959359464484823,\n        \"min\": 1.18,\n        \"max\": 2.28,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.77,\n          1.94,\n          1.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"prec\",\n        \"mean\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.8822789370382775,\n        \"min\": 59.06,\n        \"max\": 74.03,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          69.78,\n          70.07,\n          74.03\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"prec\",\n        \"std\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7839096157812365,\n        \"min\": 1.13,\n        \"max\": 3.4,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2.37,\n          2.44,\n          3.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"rec\",\n        \"mean\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.4346288259816316,\n        \"min\": 51.16,\n        \"max\": 65.11,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          65.11,\n          62.44,\n          57.94\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"rec\",\n        \"std\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.305552245858689,\n        \"min\": 1.27,\n        \"max\": 4.91,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.86,\n          3.83,\n          2.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"f1\",\n        \"mean\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.9763317625311765,\n        \"min\": 57.34,\n        \"max\": 67.37,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          67.37,\n          66.01,\n          64.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": [\n        \"f1\",\n        \"std\"\n      ],\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6714022284599243,\n        \"min\": 1.59,\n        \"max\": 3.61,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          2.0,\n          2.86,\n          2.34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":30}]}]}